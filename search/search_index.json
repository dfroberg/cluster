{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lab Cluster \u00b6 This repository is my lab Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a Github issue or join the k8s@home Discord if you have any questions. This repository is built off the k8s-at-home/template-cluster-k3s repository. Cluster setup \u00b6 My cluster is k3s provisioned overtop Ubuntu 21.04 using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes while I have a separate server for (NFS) file storage. See my ansible directory for my playbooks and roles. Cluster components \u00b6 calico : For internal cluster networking using BGP configured on Opnsense. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. external-dns : Creates DNS entries in a separate coredns deployment which is backed by my clusters etcd deployment. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt. kube-vip : HA solution for Kubernetes control plane Kasten : Data backup and recovery Repository structure \u00b6 The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. base directory is the entrypoint to Flux init directory contains SOPS and other secrets inittialization. crds directory contains custom resource definitions (CRDs) that need to exist globally in your cluster before anything else exists core directory (depends on crds ) are important infrastructure applications (grouped by namespace) that should never be pruned by Flux apps directory (depends on core ) is where your common applications (grouped by namespace) could be placed, Flux will prune resources here if they are not tracked by Git anymore ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u251c\u2500\u2500 ./init \u2514\u2500\u2500 ./crds Automate all the things! \u00b6 Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date Hardware \u00b6 Device Count OS Disk Size Data Disk Size Ram Purpose HP DL580 3 256GB SSD N/A 196GB k3s Masters (embedded etcd) HP DL360 3 240GB SSD N/A 196GB k3s Workers HP DL380 3 240GB SSD 1TB NVMe (rook-ceph) 196GB k3s Workers (label: storage=yes) TrueNAS Core 1 120GB SSD 8x2TB RAIDz2 32GB Shared file storage Tools \u00b6 Tool Purpose direnv Sets environment variable based on present working directory go-task Alternative to makefiles, who honestly likes that? pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes Thanks \u00b6 A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes","title":"Introduction"},{"location":"#lab-cluster","text":"This repository is my lab Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a Github issue or join the k8s@home Discord if you have any questions. This repository is built off the k8s-at-home/template-cluster-k3s repository.","title":"Lab Cluster"},{"location":"#cluster-setup","text":"My cluster is k3s provisioned overtop Ubuntu 21.04 using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes while I have a separate server for (NFS) file storage. See my ansible directory for my playbooks and roles.","title":"Cluster setup"},{"location":"#cluster-components","text":"calico : For internal cluster networking using BGP configured on Opnsense. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. external-dns : Creates DNS entries in a separate coredns deployment which is backed by my clusters etcd deployment. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt. kube-vip : HA solution for Kubernetes control plane Kasten : Data backup and recovery","title":"Cluster components"},{"location":"#repository-structure","text":"The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. base directory is the entrypoint to Flux init directory contains SOPS and other secrets inittialization. crds directory contains custom resource definitions (CRDs) that need to exist globally in your cluster before anything else exists core directory (depends on crds ) are important infrastructure applications (grouped by namespace) that should never be pruned by Flux apps directory (depends on core ) is where your common applications (grouped by namespace) could be placed, Flux will prune resources here if they are not tracked by Git anymore ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u251c\u2500\u2500 ./init \u2514\u2500\u2500 ./crds","title":"Repository structure"},{"location":"#automate-all-the-things","text":"Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Automate all the things!"},{"location":"#hardware","text":"Device Count OS Disk Size Data Disk Size Ram Purpose HP DL580 3 256GB SSD N/A 196GB k3s Masters (embedded etcd) HP DL360 3 240GB SSD N/A 196GB k3s Workers HP DL380 3 240GB SSD 1TB NVMe (rook-ceph) 196GB k3s Workers (label: storage=yes) TrueNAS Core 1 120GB SSD 8x2TB RAIDz2 32GB Shared file storage","title":"Hardware"},{"location":"#tools","text":"Tool Purpose direnv Sets environment variable based on present working directory go-task Alternative to makefiles, who honestly likes that? pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes","title":"Tools"},{"location":"#thanks","text":"A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes","title":"Thanks"},{"location":"installation/bootstrap-applications/","text":"Bootstrapping Applications \u00b6 The Kubernetes @Home community has created a wonderful Github template for bootstrapping a cluster for flux this can be viewed here . Bootstrapping Flux \u00b6 Create or locate cluster GPG key \u00b6 export GPG_TTY=$(tty) gpg --list-secret-keys \"Home cluster (Flux) <email>\" export FLUX_KEY_FP=ABCDEFGHIJKLMNOPQRSTUVWXYZ Verify cluster is ready for Flux \u00b6 flux --kubeconfig=./kubeconfig check --pre Pre-create the flux-system namespace \u00b6 kubectl --kubeconfig=./kubeconfig create namespace flux-system --dry-run=client -o yaml | kubectl --kubeconfig=./kubeconfig apply -f - Add the Flux GPG key in-order for Flux to decrypt SOPS secrets \u00b6 gpg --export-secret-keys --armor \"${FLUX_KEY_FP}\" | kubectl --kubeconfig=./kubeconfig create secret generic sops-gpg \\ --namespace=flux-system \\ --from-file=sops.asc=/dev/stdin Install Flux \u00b6 Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig=./kubeconfig apply --kustomize=./cluster/base/flux-system :tada: at this point after reconciliation Flux state should be restored.","title":"Bootstrap Applications"},{"location":"installation/bootstrap-applications/#bootstrapping-applications","text":"The Kubernetes @Home community has created a wonderful Github template for bootstrapping a cluster for flux this can be viewed here .","title":"Bootstrapping Applications"},{"location":"installation/bootstrap-applications/#bootstrapping-flux","text":"","title":"Bootstrapping Flux"},{"location":"installation/bootstrap-applications/#create-or-locate-cluster-gpg-key","text":"export GPG_TTY=$(tty) gpg --list-secret-keys \"Home cluster (Flux) <email>\" export FLUX_KEY_FP=ABCDEFGHIJKLMNOPQRSTUVWXYZ","title":"Create or locate cluster GPG key"},{"location":"installation/bootstrap-applications/#verify-cluster-is-ready-for-flux","text":"flux --kubeconfig=./kubeconfig check --pre","title":"Verify cluster is ready for Flux"},{"location":"installation/bootstrap-applications/#pre-create-the-flux-system-namespace","text":"kubectl --kubeconfig=./kubeconfig create namespace flux-system --dry-run=client -o yaml | kubectl --kubeconfig=./kubeconfig apply -f -","title":"Pre-create the flux-system namespace"},{"location":"installation/bootstrap-applications/#add-the-flux-gpg-key-in-order-for-flux-to-decrypt-sops-secrets","text":"gpg --export-secret-keys --armor \"${FLUX_KEY_FP}\" | kubectl --kubeconfig=./kubeconfig create secret generic sops-gpg \\ --namespace=flux-system \\ --from-file=sops.asc=/dev/stdin","title":"Add the Flux GPG key in-order for Flux to decrypt SOPS secrets"},{"location":"installation/bootstrap-applications/#install-flux","text":"Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig=./kubeconfig apply --kustomize=./cluster/base/flux-system :tada: at this point after reconciliation Flux state should be restored.","title":"Install Flux"},{"location":"installation/guidelines/","text":"Guidelines \u00b6 Here are several suggestions I have prior to installing Kubernetes or just general use. Some of these suggestions may only apply to Ubuntu. Storage \u00b6 Do not use NFS for application configuration data if that application uses SQLite with write ahead logging (WAL), or uses file locks. Applications like Sonarr, Radarr, Lidarr are clear examples to avoid using NFS for the configuration volume. Understand the importance, capabilities and limitations between file, block and object type storage. Networking \u00b6 Configure DNS on your nodes to use an upstream provider (e.g. 1.1.1.1 , 9.9.9.9 ), or your router's IP if you have DNS configured there and it's not pointing to a local adblocker DNS. Do not use a Ad-blockers (PiHole, Adguard-Home, Blocky, etc.) DNS server for your k8s nodes. Ad-blockers should be used on devices with a web browser. Remove any search domains from your hosts /etc/resolv.conf . Search domains have an issue with alpine based containers and DNS might not resolve in them. Ensure you are using iptables in nf_tables mode. Enable packet forwarding on the hosts, and apply other sysctl tweaks: cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 fs.inotify.max_user_watches=65536 EOF sudo sysctl --system Make sure your nodes hostname appears in /etc/hosts , for example: 127.0.0.1 localhost 127.0.1.1 k8s-0 System \u00b6 For a trade-off in speed over security, disable AppArmor and Mitigations on Ubuntu: # /etc/default/grub GRUB_CMDLINE_LINUX=\"apparmor=0 mitigations=off\" and then reconfigure grub and reboot: sudo update-grub sudo reboot Setup unattended-upgrade for use with Kured to automatically patch and reboot your nodes. Disable swap","title":"Guidelines"},{"location":"installation/guidelines/#guidelines","text":"Here are several suggestions I have prior to installing Kubernetes or just general use. Some of these suggestions may only apply to Ubuntu.","title":"Guidelines"},{"location":"installation/guidelines/#storage","text":"Do not use NFS for application configuration data if that application uses SQLite with write ahead logging (WAL), or uses file locks. Applications like Sonarr, Radarr, Lidarr are clear examples to avoid using NFS for the configuration volume. Understand the importance, capabilities and limitations between file, block and object type storage.","title":"Storage"},{"location":"installation/guidelines/#networking","text":"Configure DNS on your nodes to use an upstream provider (e.g. 1.1.1.1 , 9.9.9.9 ), or your router's IP if you have DNS configured there and it's not pointing to a local adblocker DNS. Do not use a Ad-blockers (PiHole, Adguard-Home, Blocky, etc.) DNS server for your k8s nodes. Ad-blockers should be used on devices with a web browser. Remove any search domains from your hosts /etc/resolv.conf . Search domains have an issue with alpine based containers and DNS might not resolve in them. Ensure you are using iptables in nf_tables mode. Enable packet forwarding on the hosts, and apply other sysctl tweaks: cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 fs.inotify.max_user_watches=65536 EOF sudo sysctl --system Make sure your nodes hostname appears in /etc/hosts , for example: 127.0.0.1 localhost 127.0.1.1 k8s-0","title":"Networking"},{"location":"installation/guidelines/#system","text":"For a trade-off in speed over security, disable AppArmor and Mitigations on Ubuntu: # /etc/default/grub GRUB_CMDLINE_LINUX=\"apparmor=0 mitigations=off\" and then reconfigure grub and reboot: sudo update-grub sudo reboot Setup unattended-upgrade for use with Kured to automatically patch and reboot your nodes. Disable swap","title":"System"},{"location":"installation/installing-kubernetes/","text":"Installing Kubernetes \u00b6 Update Ansible inventory configuration and run the k3s-install playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/k3s-prepare.yml","title":"Installing Kubernetes"},{"location":"installation/installing-kubernetes/#installing-kubernetes","text":"Update Ansible inventory configuration and run the k3s-install playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/k3s-prepare.yml","title":"Installing Kubernetes"},{"location":"installation/preparing-nodes/","text":"Preparing Nodes \u00b6 Install Ubuntu \u00b6 Download Ubuntu Server 21.04 ISO and flash it to a USB drive, boot the device from the USB drive and install Ubuntu Copy over SSH key from the machine running Ansible \u00b6 ssh-copy-id ubuntu@192.168.10.10 Configure static IPs \u00b6 Set a static IP on your nodes or you may run into issues with Alpine containers # /etc/netplan/00-installer-config.yaml network: ethernets: eno1: addresses: - 192.168.3.10/24 gateway4: 192.168.3.1 nameservers: addresses: - 192.168.3.1 search: [] version: 2 Prepare Ubuntu for k8s \u00b6 Update Ansible inventory configuration and run the ubuntu-prepare playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/ubuntu-prepare.yml","title":"Preparing Nodes"},{"location":"installation/preparing-nodes/#preparing-nodes","text":"","title":"Preparing Nodes"},{"location":"installation/preparing-nodes/#install-ubuntu","text":"Download Ubuntu Server 21.04 ISO and flash it to a USB drive, boot the device from the USB drive and install Ubuntu","title":"Install Ubuntu"},{"location":"installation/preparing-nodes/#copy-over-ssh-key-from-the-machine-running-ansible","text":"ssh-copy-id ubuntu@192.168.10.10","title":"Copy over SSH key from the machine running Ansible"},{"location":"installation/preparing-nodes/#configure-static-ips","text":"Set a static IP on your nodes or you may run into issues with Alpine containers # /etc/netplan/00-installer-config.yaml network: ethernets: eno1: addresses: - 192.168.3.10/24 gateway4: 192.168.3.1 nameservers: addresses: - 192.168.3.1 search: [] version: 2","title":"Configure static IPs"},{"location":"installation/preparing-nodes/#prepare-ubuntu-for-k8s","text":"Update Ansible inventory configuration and run the ubuntu-prepare playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/ubuntu-prepare.yml","title":"Prepare Ubuntu for k8s"},{"location":"networking/dns/","text":"DNS \u00b6 My DNS setup may seem a bit complicated at first, but it allows for completely automatic management of DNS entries for Services and Ingress objects. Components \u00b6 Traefik \u00b6 Traefik is my cluster Ingress controller. It is set to a externalIPs so that I can forward a port on my router directly to the Service. CoreDNS with k8s_gateway \u00b6 CoreDNS is running on my OPNsense router. I have included the k8s_gateway plugin so that I can connect it directly to my cluster. external-dns \u00b6 external-dns runs in my cluster and is connected to my domains DNS server. It automatically manages records for all my Ingresses that have the external-dns/is-public: \"true\" annotation set. Dynamic DNS \u00b6 In order to keep my WAN IP address up to date on my DNS provider I have deployed a CronJob ( link ) in my cluster that periodically checks and updates those records. How it all works together \u00b6 When I am connected to my home network, my DNS server is set to Blocky . I have configured this to forward all requests for my own domain names to the CoreDNS instance that is running on my router. If an Ingress or Service exists for the requested address, k8s_gateway will respond with the IP address that it received from my cluster. If it doesn't exist, it will respond with NXDOMAIN . When I am outside my home network, I will probably use whatever DNS is provided to me. When I request an address for one of my domains, it will query my domains DNS server and will respond with the DNS record that was set by external-dns .","title":"DNS"},{"location":"networking/dns/#dns","text":"My DNS setup may seem a bit complicated at first, but it allows for completely automatic management of DNS entries for Services and Ingress objects.","title":"DNS"},{"location":"networking/dns/#components","text":"","title":"Components"},{"location":"networking/dns/#traefik","text":"Traefik is my cluster Ingress controller. It is set to a externalIPs so that I can forward a port on my router directly to the Service.","title":"Traefik"},{"location":"networking/dns/#coredns-with-k8s_gateway","text":"CoreDNS is running on my OPNsense router. I have included the k8s_gateway plugin so that I can connect it directly to my cluster.","title":"CoreDNS with k8s_gateway"},{"location":"networking/dns/#external-dns","text":"external-dns runs in my cluster and is connected to my domains DNS server. It automatically manages records for all my Ingresses that have the external-dns/is-public: \"true\" annotation set.","title":"external-dns"},{"location":"networking/dns/#dynamic-dns","text":"In order to keep my WAN IP address up to date on my DNS provider I have deployed a CronJob ( link ) in my cluster that periodically checks and updates those records.","title":"Dynamic DNS"},{"location":"networking/dns/#how-it-all-works-together","text":"When I am connected to my home network, my DNS server is set to Blocky . I have configured this to forward all requests for my own domain names to the CoreDNS instance that is running on my router. If an Ingress or Service exists for the requested address, k8s_gateway will respond with the IP address that it received from my cluster. If it doesn't exist, it will respond with NXDOMAIN . When I am outside my home network, I will probably use whatever DNS is provided to me. When I request an address for one of my domains, it will query my domains DNS server and will respond with the DNS record that was set by external-dns .","title":"How it all works together"},{"location":"networking/general/","text":"Networking \u00b6 My current cluster-internal networking is implemented by calico . Name CIDR Management 192.168.10.0/24 Servers 192.168.3.0/24 k8s external services (BGP) 192.168.30.0/24 k8s pods 10.69.0.0/16 k8s services 10.96.0.0/16 Running high-available control-plane \u00b6 Warning Due to the way that BGP works, a node can only set up a single BGP connection to the router. This mean kube-vip and Calico services must not be running on the same node. In order to expose my control-plane on a loadbalanced IP address, I have deployed kube-vip . It is configured to expose a load balanced address to the host IP addresses of my control-plane nodes over BGP. Exposing services on their own IP address \u00b6 Warning Currently when using BGP on Opnsense, services do not get properly load balanced. This is due to Opnsense not supporting multipath in the BSD kernel. Most (http/https) traffic enters my cluster through an Ingress controller. For situations where this is not desirable (e.g. MQTT traffic) or when I need a fixed IP reachable from outside the cluster (e.g. to use in combination with port forwarding) I use calico configured with BGP. Using this setup I can define a Service to use a Load Balancer with externalIPs , and it will be exposed on my network on that given IP address. Mixed-protocol services \u00b6 I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine UDP and TCP ports on the same Service.","title":"General"},{"location":"networking/general/#networking","text":"My current cluster-internal networking is implemented by calico . Name CIDR Management 192.168.10.0/24 Servers 192.168.3.0/24 k8s external services (BGP) 192.168.30.0/24 k8s pods 10.69.0.0/16 k8s services 10.96.0.0/16","title":"Networking"},{"location":"networking/general/#running-high-available-control-plane","text":"Warning Due to the way that BGP works, a node can only set up a single BGP connection to the router. This mean kube-vip and Calico services must not be running on the same node. In order to expose my control-plane on a loadbalanced IP address, I have deployed kube-vip . It is configured to expose a load balanced address to the host IP addresses of my control-plane nodes over BGP.","title":"Running high-available control-plane"},{"location":"networking/general/#exposing-services-on-their-own-ip-address","text":"Warning Currently when using BGP on Opnsense, services do not get properly load balanced. This is due to Opnsense not supporting multipath in the BSD kernel. Most (http/https) traffic enters my cluster through an Ingress controller. For situations where this is not desirable (e.g. MQTT traffic) or when I need a fixed IP reachable from outside the cluster (e.g. to use in combination with port forwarding) I use calico configured with BGP. Using this setup I can define a Service to use a Load Balancer with externalIPs , and it will be exposed on my network on that given IP address.","title":"Exposing services on their own IP address"},{"location":"networking/general/#mixed-protocol-services","text":"I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine UDP and TCP ports on the same Service.","title":"Mixed-protocol services"},{"location":"quirks/unmanaged-workloads/","text":"Unmanaged workloads \u00b6 Say you wish to launch a manually managed workload into your cluster that you want FluxCD to ignore, add this annotation to it and it will be left alone; annotations: fluxcd.io/ignore: \"true\"","title":"Unmanaged workloads"},{"location":"quirks/unmanaged-workloads/#unmanaged-workloads","text":"Say you wish to launch a manually managed workload into your cluster that you want FluxCD to ignore, add this annotation to it and it will be left alone; annotations: fluxcd.io/ignore: \"true\"","title":"Unmanaged workloads"},{"location":"storage/kasten-dashboard/","text":"Dahsboard \u00b6 Login to k10 Obtain token; sa_secret=$(kubectl get serviceaccount k10-k10 -o jsonpath=\"{.secrets[0].name}\" --namespace kasten-io) kubectl get secret $sa_secret --namespace kasten-io -ojsonpath=\"{.data.token}{'\\n'}\" | base64 --decode && echo","title":"Dahsboard"},{"location":"storage/kasten-dashboard/#dahsboard","text":"Login to k10 Obtain token; sa_secret=$(kubectl get serviceaccount k10-k10 -o jsonpath=\"{.secrets[0].name}\" --namespace kasten-io) kubectl get secret $sa_secret --namespace kasten-io -ojsonpath=\"{.data.token}{'\\n'}\" | base64 --decode && echo","title":"Dahsboard"},{"location":"storage/kasten-data-backup/","text":"Kasten Data Backup \u00b6 Work in progress This document is a work in progress.","title":"Backing up data using Kasten"},{"location":"storage/kasten-data-backup/#kasten-data-backup","text":"Work in progress This document is a work in progress.","title":"Kasten Data Backup"},{"location":"storage/kasten-data-restore/","text":"Kasten Data Restore \u00b6 Recovering from a K10 backup involves the following sequence of actions Create k10-dr-secret Kubernetes Secret \u00b6 The <passphrase> was set during the first installation of k10 kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key=<passphrase> Install a fresh K10 instance \u00b6 Ensure that Flux has correctly deployed K10 to it's namespace kasten-io flux get hr -n kasten-io Verify the nfs storage profile was created \u00b6 kubectl get profiles -n kasten-io Restoring the K10 backup \u00b6 Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job The <source-cluster-id> was set during the first installation of k10 helm install k10-restore kasten/k10restore -n kasten-io \\ --set sourceClusterID=<source-cluster-id> \\ --set profile.name=nfs Application recovery \u00b6 Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Restoring data with Kasten"},{"location":"storage/kasten-data-restore/#kasten-data-restore","text":"Recovering from a K10 backup involves the following sequence of actions","title":"Kasten Data Restore"},{"location":"storage/kasten-data-restore/#create-k10-dr-secret-kubernetes-secret","text":"The <passphrase> was set during the first installation of k10 kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key=<passphrase>","title":"Create k10-dr-secret Kubernetes Secret"},{"location":"storage/kasten-data-restore/#install-a-fresh-k10-instance","text":"Ensure that Flux has correctly deployed K10 to it's namespace kasten-io flux get hr -n kasten-io","title":"Install a fresh K10 instance"},{"location":"storage/kasten-data-restore/#verify-the-nfs-storage-profile-was-created","text":"kubectl get profiles -n kasten-io","title":"Verify the nfs storage profile was created"},{"location":"storage/kasten-data-restore/#restoring-the-k10-backup","text":"Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job The <source-cluster-id> was set during the first installation of k10 helm install k10-restore kasten/k10restore -n kasten-io \\ --set sourceClusterID=<source-cluster-id> \\ --set profile.name=nfs","title":"Restoring the K10 backup"},{"location":"storage/kasten-data-restore/#application-recovery","text":"Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Application recovery"},{"location":"storage/manual-data-backup/","text":"Manual Data Backup \u00b6 Create the toolbox container \u00b6 Ran from your workstation kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') -- bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.3.113:/Data /mnt/nfsdata Restore data from a NFS share \u00b6 Ran from your workstation Pause the Flux Helm Release flux suspend hr home-assistant -n home Scale the application down to zero pods kubectl scale deploy/home-assistant --replicas 0 -n home Get the csi-vol-* string kubectl get pv/$(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data tar czvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data/ . umount /mnt/data rbd unmap -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa","title":"Backing up data manually"},{"location":"storage/manual-data-backup/#manual-data-backup","text":"","title":"Manual Data Backup"},{"location":"storage/manual-data-backup/#create-the-toolbox-container","text":"Ran from your workstation kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') -- bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.3.113:/Data /mnt/nfsdata","title":"Create the toolbox container"},{"location":"storage/manual-data-backup/#restore-data-from-a-nfs-share","text":"Ran from your workstation Pause the Flux Helm Release flux suspend hr home-assistant -n home Scale the application down to zero pods kubectl scale deploy/home-assistant --replicas 0 -n home Get the csi-vol-* string kubectl get pv/$(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data tar czvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data/ . umount /mnt/data rbd unmap -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa","title":"Restore data from a NFS share"},{"location":"storage/manual-data-restore/","text":"Manual Data Restore \u00b6 Create the toolbox container \u00b6 Ran from your workstation kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') -- bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.42.50:/Data /mnt/nfsdata Restore data from a NFS share \u00b6 Ran from your workstation Apply the PVC kubectl apply -f cluster/apps/home/home-assistant/config-pvc.yaml Get the csi-vol-* string kubectl get pv/$(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} sh -c 'mkfs.ext4 {}' rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} mount {} /mnt/data tar xvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data umount /mnt/data rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212","title":"Restoring data manually"},{"location":"storage/manual-data-restore/#manual-data-restore","text":"","title":"Manual Data Restore"},{"location":"storage/manual-data-restore/#create-the-toolbox-container","text":"Ran from your workstation kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') -- bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.42.50:/Data /mnt/nfsdata","title":"Create the toolbox container"},{"location":"storage/manual-data-restore/#restore-data-from-a-nfs-share","text":"Ran from your workstation Apply the PVC kubectl apply -f cluster/apps/home/home-assistant/config-pvc.yaml Get the csi-vol-* string kubectl get pv/$(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} sh -c 'mkfs.ext4 {}' rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} mount {} /mnt/data tar xvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data umount /mnt/data rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212","title":"Restore data from a NFS share"},{"location":"storage/velero/","text":"Configure Veleo \u00b6 Install the velero-cli Install GitHub release \u00b6 Download the latest release\u2019s tarball for your client platform. https://github.com/vmware-tanzu/velero/releases/latest I.e; curl -L https://github.com/vmware-tanzu/velero/releases/download/v1.6.3/velero-v1.6.3-linux-amd64.tar.gz --output velero.tar.gz Extract the tarball: tar -xvf velero.tar.gz Move the extracted velero binary to somewhere in your $PATH (/usr/local/bin for most users). sudo cp velero-v1.6.3-linux-amd64/velero /usr/local/bin/ Apply secrets: \u00b6 Clean out and re-encrypt ./cluster/apps/velero/secret.sops.yaml modify content to read; stringData: cloud: | [default] aws_access_key_id=<secret> aws_secret_access_key=<not going to tell> Check Installation \u00b6 velero version Client: Version: v1.6.3 Git commit: 5fe3a50bfddc2becb4c0bd5e2d3d4053a23e95d2 Server: Version: v1.6.3 velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero Unavailable 2021-09-13 11:21:08 +0200 CEST ReadWrite true Ooops if you see Unavailable check MinIO if you remebered to create the bucket or if s3Url is ok... Once you fixed everything: NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero Available 2021-09-13 11:28:26 +0200 CEST ReadWrite true Taaadaa Available ! Check if you're schedule is set; velero get schedules NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR velero-daily-backup Enabled 2021-09-13 11:16:59 +0200 CEST 0 6 * * * 120h0m0s 14m ago <none> Check if backups are succeding; velero get backups NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR velero-daily-backup-20210913114258 Completed 0 0 2021-09-13 11:42:58 +0200 CEST 4d default <none> Annotations \u00b6 To enable PV backups on workloads you need to describe in an annotation which volumes to backup; In manfifests add; apiVersion: v1 kind: Pod metadata: annotations: backup.velero.io/backup-volumes: data or from kubectl; Examples; kubectl -n YOUR_POD_NAMESPACE annotate pod/YOUR_POD_NAME backup.velero.io/backup-volumes=YOUR_VOLUME_NAME_1,YOUR_VOLUME_NAME_2,... kubectl -n default annotate pod hajimari-696b7f8d7d-jf8jh backup.velero.io/backup-volumes=data Backups \u00b6 Entire namespace \u00b6 velero backup create backup-demo-app-ns --include-namespaces demo-ns --wait","title":"Configure Veleo"},{"location":"storage/velero/#configure-veleo","text":"Install the velero-cli","title":"Configure Veleo"},{"location":"storage/velero/#install-github-release","text":"Download the latest release\u2019s tarball for your client platform. https://github.com/vmware-tanzu/velero/releases/latest I.e; curl -L https://github.com/vmware-tanzu/velero/releases/download/v1.6.3/velero-v1.6.3-linux-amd64.tar.gz --output velero.tar.gz Extract the tarball: tar -xvf velero.tar.gz Move the extracted velero binary to somewhere in your $PATH (/usr/local/bin for most users). sudo cp velero-v1.6.3-linux-amd64/velero /usr/local/bin/","title":"Install GitHub release"},{"location":"storage/velero/#apply-secrets","text":"Clean out and re-encrypt ./cluster/apps/velero/secret.sops.yaml modify content to read; stringData: cloud: | [default] aws_access_key_id=<secret> aws_secret_access_key=<not going to tell>","title":"Apply secrets:"},{"location":"storage/velero/#check-installation","text":"velero version Client: Version: v1.6.3 Git commit: 5fe3a50bfddc2becb4c0bd5e2d3d4053a23e95d2 Server: Version: v1.6.3 velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero Unavailable 2021-09-13 11:21:08 +0200 CEST ReadWrite true Ooops if you see Unavailable check MinIO if you remebered to create the bucket or if s3Url is ok... Once you fixed everything: NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero Available 2021-09-13 11:28:26 +0200 CEST ReadWrite true Taaadaa Available ! Check if you're schedule is set; velero get schedules NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR velero-daily-backup Enabled 2021-09-13 11:16:59 +0200 CEST 0 6 * * * 120h0m0s 14m ago <none> Check if backups are succeding; velero get backups NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR velero-daily-backup-20210913114258 Completed 0 0 2021-09-13 11:42:58 +0200 CEST 4d default <none>","title":"Check Installation"},{"location":"storage/velero/#annotations","text":"To enable PV backups on workloads you need to describe in an annotation which volumes to backup; In manfifests add; apiVersion: v1 kind: Pod metadata: annotations: backup.velero.io/backup-volumes: data or from kubectl; Examples; kubectl -n YOUR_POD_NAMESPACE annotate pod/YOUR_POD_NAME backup.velero.io/backup-volumes=YOUR_VOLUME_NAME_1,YOUR_VOLUME_NAME_2,... kubectl -n default annotate pod hajimari-696b7f8d7d-jf8jh backup.velero.io/backup-volumes=data","title":"Annotations"},{"location":"storage/velero/#backups","text":"","title":"Backups"},{"location":"storage/velero/#entire-namespace","text":"velero backup create backup-demo-app-ns --include-namespaces demo-ns --wait","title":"Entire namespace"}]}